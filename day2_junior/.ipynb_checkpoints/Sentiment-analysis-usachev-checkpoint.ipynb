{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "tweets = pandas.DataFrame.from_csv(\"Tweets data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Sentiment                                      SentimentText\n",
      "ItemID                                                              \n",
      "1               0                       is so sad for my APL frie...\n",
      "5002            1       very very very happy and slightly skinny  xx\n",
      "10012           1  &quot;I feel better now, better than James Bro...\n",
      "15012           1  ...what can I say ....what a surprise...http:/...\n",
      "20012           0                                     @ ponyp: hey! \n",
      "25012           1       @404world yes I'm going to find one..pronto \n",
      "30012           0  @AdamJonesey mmm id love some parma violets ri...\n",
      "35012           1  @AlTheYid Roast beef and wasabi... yummmmmmmmm...\n",
      "40012           0  @amandafortier ahhh. yeah. kinda. I tweet from...\n",
      "45012           0                 @antdeshawn ... the pain is worst \n",
      "50012           1  @Ashtarte Just be your usual supportive self. ...\n",
      "55012           0  @azizabatini86 and that's fine--u kno, the dry...\n",
      "60012           0       @Bellemorda I miss you!!! Get back on soon. \n",
      "65012           0  @BrandyWandLover yer so was i hun. sum guy was...\n",
      "70012           0  @Andreicaaat Aww damn I can watch your vid on ...\n",
      "75012           1                      @anorangegal You've got mail \n",
      "80012           1  @cantyka hallo juga  thanks ya udah di follow....\n",
      "85012           0  @cbhamby easy there tiger...you should have co...\n",
      "90012           1  @clickjow depois tu pode ver outras s�ries. te...\n",
      "95012           0  @cianw I've never really been able to eat muss...\n"
     ]
    }
   ],
   "source": [
    "print(tweets[::5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# print(tweets[:10])\n",
    "for i in tweets.Sentiment[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing\n",
    "Извлечём из твитов признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = tweets[\"SentimentText\"]\n",
    "Y = tweets[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    features = []\n",
    "    \n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    #<для начала будем использовать частоты букв как признаки>\n",
    "    letters = \"abcdefghijklmnopqrstuvwxyz\"    \n",
    "    for l in letters:\n",
    "        features.append(text.count(l))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features ('AAaab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "X = []\n",
    "for text in texts:\n",
    "    X.append(extract_features(text))\n",
    "X = numpy.array(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a model\n",
    "Научим на наших признаках простую линейную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#порежем данные\n",
    "X_train = X[:70000]\n",
    "Y_train = Y[:70000]\n",
    "X_test = X[70000:]\n",
    "Y_test = Y[70000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# model = <обучите логистическую регрессию>\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03976119, -0.02395792,  0.03336063, -0.07673787,  0.01470141,\n",
       "         0.04481567,  0.04607898,  0.00136932, -0.04444427,  0.14625134,\n",
       "         0.05731417,  0.04799672, -0.05269558, -0.03541793,  0.01029763,\n",
       "         0.01853502,  0.23346672,  0.02136664, -0.06216603, -0.03674036,\n",
       "         0.03427765,  0.02921815, -0.01795578,  0.06048928,  0.04883713,\n",
       "         0.06188793]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X[7000:14000], Y[7000:14000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01513508,  0.0253065 ,  0.01467999, -0.06508438, -0.02102025,\n",
       "         0.07285677,  0.01079471, -0.00807934, -0.055264  ,  0.23026256,\n",
       "         0.0582649 ,  0.03025168,  0.0808289 , -0.03890496, -0.00357455,\n",
       "         0.00564272,  0.35088587,  0.00517337, -0.01687005, -0.00699748,\n",
       "        -0.01350235,  0.09033838,  0.02882315,  0.15865924,  0.07377224,\n",
       "         0.23060619]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01513508,  0.0253065 ,  0.01467999, -0.06508438, -0.02102025,\n",
       "         0.07285677,  0.01079471, -0.00807934, -0.055264  ,  0.23026256,\n",
       "         0.0582649 ,  0.03025168,  0.0808289 , -0.03890496, -0.00357455,\n",
       "         0.00564272,  0.35088587,  0.00517337, -0.01687005, -0.00699748,\n",
       "        -0.01350235,  0.09033838,  0.02882315,  0.15865924,  0.07377224,\n",
       "         0.23060619]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict_proba() takes exactly 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-f026f10497ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: predict_proba() takes exactly 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "model.predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение:\n",
      "0.567601786052\n",
      "Тест:\n",
      "0.558523325999\n"
     ]
    }
   ],
   "source": [
    "Y_pred_train = model.predict_proba(X_train)[:,1]\n",
    "Y_pred_test = model.predict_proba(X_test)[:,1]\n",
    "print(u\"Обучение:\") \n",
    "print(roc_auc_score(Y_train,Y_pred_train))\n",
    "#recieve operator curve area under curve\n",
    "print(u\"Тест:\") \n",
    "print(roc_auc_score(Y_test,Y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# opinion mining\n",
    "\n",
    "Давайте использовуем нашу модель чтобы понять, что людям сейчас нравится/не нравится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "with open(\"testdata.txt\") as fin:\n",
    "    test_tweets = fin.read()[:-1].split('\\n')\n",
    "test_tweets = numpy.array(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\" I don\\'t care what anyone says, I like Hillary Clinton.'\n",
      " 'have an awesome time at purdue!..'\n",
      " \"Yep, I'm still in London, which is pretty awesome: P Remind me to post the million and one pictures that I took when I get back to Markham!...\"\n",
      " \"Have to say, I hate Paris Hilton's behavior but I do think she's kinda cute..\"\n",
      " 'i will love the lakers.']\n"
     ]
    }
   ],
   "source": [
    "print test_tweets[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in test_tweets[:5]:\n",
    "    p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" I don't care what anyone says, I like Hillary Clinton.\n"
     ]
    }
   ],
   "source": [
    "for i in test_tweets[:1]:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_ops = []\n",
    "for tweet in test_tweets:\n",
    "    X_ops.append(extract_features(tweet))\n",
    "X_ops = numpy.array(X_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-37fcba1343d9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-24-37fcba1343d9>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    sentiments = <предскажите эмоциональную окраску x_ops>\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sentiments = <предскажите эмоциональную окраску x_ops>\n",
    "sentiment_order = numpy.argsort(-sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### теперь посмотрим на наиболее любимые/ненавистные людям вещи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment_order' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-2bb5c6d2861a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentiment_order\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentiment_order' is not defined"
     ]
    }
   ],
   "source": [
    "best_ids = sentiment_order[:10]\n",
    "\n",
    "for tweet, score in zip(test_tweets[best_ids], sentiments[best_ids]):\n",
    "    print score,':',tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment_order' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-f833962b7abd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mworst_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentiment_order\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mworst_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mworst_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentiment_order' is not defined"
     ]
    }
   ],
   "source": [
    "worst_ids = sentiment_order[-10:]\n",
    "\n",
    "for tweet, score in zip(test_tweets[worst_ids], sentiments[worst_ids]):\n",
    "    print score,':',tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "stemmer = nltk.snowball.RussianStemmer()\n",
    "re_tokenizer = nltk.tokenize.RegexpTokenizer(r'[А-Яа-яA-Za-z0-9\\']+')\n",
    "text_words=[]\n",
    "for i in test_tweets: \n",
    "    temp=re_tokenizer.tokenize(i)\n",
    "    for y in temp:\n",
    "        text_words.append (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"don't\", 'care', 'what', 'anyone']\n"
     ]
    }
   ],
   "source": [
    "print (text_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counters = Counter(text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raining\n",
      "4\n",
      "Uni\n",
      "1\n",
      "yellow\n",
      "6\n",
      "CBR600RR\n",
      "1\n",
      "four\n",
      "11\n",
      "prices\n",
      "4\n",
      "Does\n",
      "4\n",
      "Olympics\n",
      "1\n",
      "friend's\n",
      "1\n",
      "hanging\n",
      "5\n",
      "centimeter\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "l=0\n",
    "for i in counters:\n",
    "    print i\n",
    "    print counters[i]\n",
    "    l+=1\n",
    "    if l>10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = [w for w, cnt in counters.most_common(100)]\n",
    "word_to_id = {word:i for i,word in enumerate(list(words))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "the\n",
      "i\n",
      "and\n",
      "to\n",
      "love\n",
      "all\n",
      "83\n",
      "just\n",
      "88\n",
      "Paris\n",
      "47\n",
      "london\n",
      "99\n",
      "go\n",
      "92\n",
      "still\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "l=0\n",
    "for i in words:\n",
    "    print i\n",
    "    l+=1\n",
    "    if l>5: break\n",
    "        \n",
    "l=0\n",
    "for i in word_to_id:\n",
    "    print i\n",
    "    print word_to_id[i]\n",
    "    l+=1\n",
    "    if l>5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#выделите 1000 cлов, которые имеют наиболее разную вероятность войти в позитивный и негативный твит\n",
    "bag_of_words = <слова-признаки>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    features = numpy.zeros(100)\n",
    "    \n",
    "    words=re_tokenizer.tokenize(text)\n",
    "    \n",
    "    \n",
    "    for i in word_to_id.keys():\n",
    "        if i in words:    \n",
    "            features[word_to_id[i]]=1\n",
    "        else:\n",
    "            features[word_to_id[i]]=0\n",
    "    \n",
    "    return numpy.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "X = []\n",
    "for text in texts:\n",
    "    X.append(extract_features(text))\n",
    "X = numpy.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "modelW = LogisticRegression()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelW.fit(X[:70000], Y[:70000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = X[:70000]\n",
    "Y_train = Y[:70000]\n",
    "X_test = X[70000:]\n",
    "Y_test = Y[70000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48043097,  0.1307492 , -0.6869916 ,  0.06659804, -0.08715588,\n",
       "         1.05551664, -0.09387772,  0.20143918, -0.31857099,  0.01000838,\n",
       "        -0.09726974,  0.04799031, -1.51054736, -0.41014664,  0.13781958,\n",
       "        -0.21389823, -0.22958884,  0.25187863, -1.1159351 ,  0.07265507,\n",
       "        -0.22837775,  0.68373938,  1.1564716 , -0.06377541,  0.05292679,\n",
       "        -0.21719257, -0.20272343, -2.55011768, -0.64357111,  0.26707048,\n",
       "         0.04686916, -0.42564515,  0.23137983,  0.59255256, -0.62076144,\n",
       "         0.11536619,  0.13215331,  0.60576724, -0.53214702,  0.1193036 ,\n",
       "         0.53850964,  0.55689599,  0.24486757,  0.01599564, -0.13809072,\n",
       "        -0.1016285 , -0.10636934,  0.08281296,  0.        , -0.80758178,\n",
       "        -0.2016889 , -0.02871098,  0.01478995,  0.27889685, -0.10749062,\n",
       "        -0.67504535,  0.4831555 ,  0.00732276, -0.02257154,  1.04127425,\n",
       "        -1.6226956 , -0.37911592, -0.22720661, -0.15834625, -0.17046448,\n",
       "        -0.42313468, -0.55408508,  0.21777448,  0.39404438,  0.05705044,\n",
       "         1.06641379,  0.09417537,  0.17219274,  0.46670984,  0.46767277,\n",
       "        -0.82479661,  0.46983746,  0.07853868, -0.51553506,  0.33159213,\n",
       "        -0.57520293,  0.03767723, -0.50341511,  0.01113644, -0.41205217,\n",
       "        -0.16066159, -0.23932812, -0.05108676,  0.11654236,  0.01918447,\n",
       "         0.34360786,  0.        , -0.43512843,  0.        ,  0.05471248,\n",
       "         0.04723773,  0.03305283, -0.00777836, -0.49876418, -0.3365847 ]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelW.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение:\n",
      "0.684341336656\n",
      "Тест:\n",
      "0.685823367813\n"
     ]
    }
   ],
   "source": [
    "Y_pred_train = modelW.predict_proba(X_train)[:,1]\n",
    "Y_pred_test = modelW.predict_proba(X_test)[:,1]\n",
    "print(u\"Обучение:\") \n",
    "print(roc_auc_score(Y_train,Y_pred_train))\n",
    "#recieve operator curve area under curve\n",
    "print(u\"Тест:\") \n",
    "print(roc_auc_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred_train = <предскажите вероятности для обучающей выборки copy-paste>\n",
    "Y_pred_test = <предскажите вероятности для обучающей выборки copy-paste>\n",
    "print(\"Обучение:\",roc_auc_score(Y_train,Y_pred_train))\n",
    "print(\"Тест:\",roc_auc_score(Y_test,Y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# collocations\n",
    "Найдём наиболее частые коллокации и используем их как признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfinder = collocations.BigramCollocationFinder.from_words(text_words)\n",
    "cfinder.apply_freq_filter(5)\n",
    "measure = collocations.BigramAssocMeasures.raw_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cfinder.nbest(measure,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    features = []\n",
    "    \n",
    "    <bag of words для коллокаций>\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "X = []\n",
    "for text in texts:\n",
    "    X.append(extract_features(text))\n",
    "X = numpy.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = <обучите модель>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred_train = <предскажите вероятности для обучающей выборки copy-paste>\n",
    "Y_pred_test = <предскажите вероятности для обучающей выборки copy-paste>\n",
    "print(\"Обучение:\",roc_auc_score(Y_train,Y_pred_train))\n",
    "print(\"Тест:\",roc_auc_score(Y_test,Y_pred_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
