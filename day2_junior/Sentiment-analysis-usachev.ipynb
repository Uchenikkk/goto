{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "tweets = pandas.DataFrame.from_csv(\"Tweets data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Sentiment                                      SentimentText\n",
      "ItemID                                                              \n",
      "1               0                       is so sad for my APL frie...\n",
      "5002            1       very very very happy and slightly skinny  xx\n",
      "10012           1  &quot;I feel better now, better than James Bro...\n",
      "15012           1  ...what can I say ....what a surprise...http:/...\n",
      "20012           0                                     @ ponyp: hey! \n",
      "25012           1       @404world yes I'm going to find one..pronto \n",
      "30012           0  @AdamJonesey mmm id love some parma violets ri...\n",
      "35012           1  @AlTheYid Roast beef and wasabi... yummmmmmmmm...\n",
      "40012           0  @amandafortier ahhh. yeah. kinda. I tweet from...\n",
      "45012           0                 @antdeshawn ... the pain is worst \n",
      "50012           1  @Ashtarte Just be your usual supportive self. ...\n",
      "55012           0  @azizabatini86 and that's fine--u kno, the dry...\n",
      "60012           0       @Bellemorda I miss you!!! Get back on soon. \n",
      "65012           0  @BrandyWandLover yer so was i hun. sum guy was...\n",
      "70012           0  @Andreicaaat Aww damn I can watch your vid on ...\n",
      "75012           1                      @anorangegal You've got mail \n",
      "80012           1  @cantyka hallo juga  thanks ya udah di follow....\n",
      "85012           0  @cbhamby easy there tiger...you should have co...\n",
      "90012           1  @clickjow depois tu pode ver outras s�ries. te...\n",
      "95012           0  @cianw I've never really been able to eat muss...\n"
     ]
    }
   ],
   "source": [
    "print(tweets[::5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# print(tweets[:10])\n",
    "for i in tweets.Sentiment[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing\n",
    "Извлечём из твитов признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = tweets[\"SentimentText\"]\n",
    "Y = tweets[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    features = []\n",
    "    \n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    #<для начала будем использовать частоты букв как признаки>\n",
    "    letters = \"abcdefghijklmnopqrstuvwxyz\"    \n",
    "    for l in letters:\n",
    "        features.append(text.count(l))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features ('AAaab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "X = []\n",
    "for text in texts:\n",
    "    X.append(extract_features(text))\n",
    "X = numpy.array(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a model\n",
    "Научим на наших признаках простую линейную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#порежем данные\n",
    "X_train = X[:70000]\n",
    "Y_train = Y[:70000]\n",
    "X_test = X[70000:]\n",
    "Y_test = Y[70000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# model = <обучите логистическую регрессию>\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03976119, -0.02395792,  0.03336063, -0.07673787,  0.01470141,\n",
       "         0.04481567,  0.04607898,  0.00136932, -0.04444427,  0.14625134,\n",
       "         0.05731417,  0.04799672, -0.05269558, -0.03541793,  0.01029763,\n",
       "         0.01853502,  0.23346672,  0.02136664, -0.06216603, -0.03674036,\n",
       "         0.03427765,  0.02921815, -0.01795578,  0.06048928,  0.04883713,\n",
       "         0.06188793]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X[7000:14000], Y[7000:14000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01513508,  0.0253065 ,  0.01467999, -0.06508438, -0.02102025,\n",
       "         0.07285677,  0.01079471, -0.00807934, -0.055264  ,  0.23026256,\n",
       "         0.0582649 ,  0.03025168,  0.0808289 , -0.03890496, -0.00357455,\n",
       "         0.00564272,  0.35088587,  0.00517337, -0.01687005, -0.00699748,\n",
       "        -0.01350235,  0.09033838,  0.02882315,  0.15865924,  0.07377224,\n",
       "         0.23060619]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01513508,  0.0253065 ,  0.01467999, -0.06508438, -0.02102025,\n",
       "         0.07285677,  0.01079471, -0.00807934, -0.055264  ,  0.23026256,\n",
       "         0.0582649 ,  0.03025168,  0.0808289 , -0.03890496, -0.00357455,\n",
       "         0.00564272,  0.35088587,  0.00517337, -0.01687005, -0.00699748,\n",
       "        -0.01350235,  0.09033838,  0.02882315,  0.15865924,  0.07377224,\n",
       "         0.23060619]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict_proba() takes exactly 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-f026f10497ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: predict_proba() takes exactly 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "model.predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение:\n",
      "0.567601786052\n",
      "Тест:\n",
      "0.558523325999\n"
     ]
    }
   ],
   "source": [
    "Y_pred_train = model.predict_proba(X_train)[:,1]\n",
    "Y_pred_test = model.predict_proba(X_test)[:,1]\n",
    "print(u\"Обучение:\") \n",
    "print(roc_auc_score(Y_train,Y_pred_train))\n",
    "#recieve operator curve area under curve\n",
    "print(u\"Тест:\") \n",
    "print(roc_auc_score(Y_test,Y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# opinion mining\n",
    "\n",
    "Давайте использовуем нашу модель чтобы понять, что людям сейчас нравится/не нравится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "with open(\"testdata.txt\") as fin:\n",
    "    test_tweets = fin.read()[:-1].split('\\n')\n",
    "test_tweets = numpy.array(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\" I don\\'t care what anyone says, I like Hillary Clinton.'\n",
      " 'have an awesome time at purdue!..'\n",
      " \"Yep, I'm still in London, which is pretty awesome: P Remind me to post the million and one pictures that I took when I get back to Markham!...\"\n",
      " \"Have to say, I hate Paris Hilton's behavior but I do think she's kinda cute..\"\n",
      " 'i will love the lakers.']\n"
     ]
    }
   ],
   "source": [
    "print test_tweets[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in test_tweets[:5]:\n",
    "    p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" I don't care what anyone says, I like Hillary Clinton.\n"
     ]
    }
   ],
   "source": [
    "for i in test_tweets[:1]:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_ops = []\n",
    "for tweet in test_tweets:\n",
    "    X_ops.append(extract_features(tweet))\n",
    "X_ops = numpy.array(X_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-37fcba1343d9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-24-37fcba1343d9>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    sentiments = <предскажите эмоциональную окраску x_ops>\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sentiments = <предскажите эмоциональную окраску x_ops>\n",
    "sentiment_order = numpy.argsort(-sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### теперь посмотрим на наиболее любимые/ненавистные людям вещи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment_order' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-2bb5c6d2861a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentiment_order\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentiment_order' is not defined"
     ]
    }
   ],
   "source": [
    "best_ids = sentiment_order[:10]\n",
    "\n",
    "for tweet, score in zip(test_tweets[best_ids], sentiments[best_ids]):\n",
    "    print score,':',tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment_order' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-f833962b7abd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mworst_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentiment_order\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mworst_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mworst_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentiment_order' is not defined"
     ]
    }
   ],
   "source": [
    "worst_ids = sentiment_order[-10:]\n",
    "\n",
    "for tweet, score in zip(test_tweets[worst_ids], sentiments[worst_ids]):\n",
    "    print score,':',tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "stemmer = nltk.snowball.RussianStemmer()\n",
    "re_tokenizer = nltk.tokenize.RegexpTokenizer(r'[А-Яа-яA-Za-z0-9\\']+')\n",
    "text_words=[]\n",
    "for i in test_tweets: \n",
    "    temp=re_tokenizer.tokenize(i)\n",
    "    for y in temp:\n",
    "        text_words.append (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"don't\", 'care', 'what', 'anyone']\n"
     ]
    }
   ],
   "source": [
    "print (text_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counters = Counter(text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raining\n",
      "4\n",
      "Uni\n",
      "1\n",
      "yellow\n",
      "6\n",
      "CBR600RR\n",
      "1\n",
      "four\n",
      "11\n",
      "prices\n",
      "4\n",
      "Does\n",
      "4\n",
      "Olympics\n",
      "1\n",
      "friend's\n",
      "1\n",
      "hanging\n",
      "5\n",
      "centimeter\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "l=0\n",
    "for i in counters:\n",
    "    print i\n",
    "    print counters[i]\n",
    "    l+=1\n",
    "    if l>10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = [w for w, cnt in counters.most_common(100)]\n",
    "word_to_id = {word:i for i,word in enumerate(list(words))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "the\n",
      "i\n",
      "and\n",
      "to\n",
      "love\n",
      "all\n",
      "83\n",
      "just\n",
      "88\n",
      "Paris\n",
      "47\n",
      "london\n",
      "99\n",
      "go\n",
      "92\n",
      "still\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "l=0\n",
    "for i in words:\n",
    "    print i\n",
    "    l+=1\n",
    "    if l>5: break\n",
    "        \n",
    "l=0\n",
    "for i in word_to_id:\n",
    "    print i\n",
    "    print word_to_id[i]\n",
    "    l+=1\n",
    "    if l>5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#выделите 1000 cлов, которые имеют наиболее разную вероятность войти в позитивный и негативный твит\n",
    "bag_of_words = <слова-признаки>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    features = numpy.zeros(100)\n",
    "    \n",
    "    words=re_tokenizer.tokenize(text)\n",
    "    \n",
    "    \n",
    "    for i in word_to_id.keys():\n",
    "        if i in words:    \n",
    "            features[word_to_id[i]]=1\n",
    "        else:\n",
    "            features[word_to_id[i]]=0\n",
    "    \n",
    "    return numpy.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "X = []\n",
    "for text in texts:\n",
    "    X.append(extract_features(text))\n",
    "X = numpy.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "modelW = LogisticRegression()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelW.fit(X[:70000], Y[:70000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = X[:70000]\n",
    "Y_train = Y[:70000]\n",
    "X_test = X[70000:]\n",
    "Y_test = Y[70000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48043097,  0.1307492 , -0.6869916 ,  0.06659804, -0.08715588,\n",
       "         1.05551664, -0.09387772,  0.20143918, -0.31857099,  0.01000838,\n",
       "        -0.09726974,  0.04799031, -1.51054736, -0.41014664,  0.13781958,\n",
       "        -0.21389823, -0.22958884,  0.25187863, -1.1159351 ,  0.07265507,\n",
       "        -0.22837775,  0.68373938,  1.1564716 , -0.06377541,  0.05292679,\n",
       "        -0.21719257, -0.20272343, -2.55011768, -0.64357111,  0.26707048,\n",
       "         0.04686916, -0.42564515,  0.23137983,  0.59255256, -0.62076144,\n",
       "         0.11536619,  0.13215331,  0.60576724, -0.53214702,  0.1193036 ,\n",
       "         0.53850964,  0.55689599,  0.24486757,  0.01599564, -0.13809072,\n",
       "        -0.1016285 , -0.10636934,  0.08281296,  0.        , -0.80758178,\n",
       "        -0.2016889 , -0.02871098,  0.01478995,  0.27889685, -0.10749062,\n",
       "        -0.67504535,  0.4831555 ,  0.00732276, -0.02257154,  1.04127425,\n",
       "        -1.6226956 , -0.37911592, -0.22720661, -0.15834625, -0.17046448,\n",
       "        -0.42313468, -0.55408508,  0.21777448,  0.39404438,  0.05705044,\n",
       "         1.06641379,  0.09417537,  0.17219274,  0.46670984,  0.46767277,\n",
       "        -0.82479661,  0.46983746,  0.07853868, -0.51553506,  0.33159213,\n",
       "        -0.57520293,  0.03767723, -0.50341511,  0.01113644, -0.41205217,\n",
       "        -0.16066159, -0.23932812, -0.05108676,  0.11654236,  0.01918447,\n",
       "         0.34360786,  0.        , -0.43512843,  0.        ,  0.05471248,\n",
       "         0.04723773,  0.03305283, -0.00777836, -0.49876418, -0.3365847 ]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelW.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение:\n",
      "0.684341336656\n",
      "Тест:\n",
      "0.685823367813\n"
     ]
    }
   ],
   "source": [
    "Y_pred_train = modelW.predict_proba(X_train)[:,1]\n",
    "Y_pred_test = modelW.predict_proba(X_test)[:,1]\n",
    "print(u\"Обучение:\") \n",
    "print(roc_auc_score(Y_train,Y_pred_train))\n",
    "#recieve operator curve area under curve\n",
    "print(u\"Тест:\") \n",
    "print(roc_auc_score(Y_test,Y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred_train = <предскажите вероятности для обучающей выборки copy-paste>\n",
    "Y_pred_test = <предскажите вероятности для обучающей выборки copy-paste>\n",
    "print(\"Обучение:\",roc_auc_score(Y_train,Y_pred_train))\n",
    "print(\"Тест:\",roc_auc_score(Y_test,Y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# collocations\n",
    "Найдём наиболее частые коллокации и используем их как признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_words= [i.lower() for i in text_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " \"don't\",\n",
       " 'care',\n",
       " 'what',\n",
       " 'anyone',\n",
       " 'says',\n",
       " 'i',\n",
       " 'like',\n",
       " 'hillary',\n",
       " 'clinton',\n",
       " 'have',\n",
       " 'an',\n",
       " 'awesome',\n",
       " 'time',\n",
       " 'at',\n",
       " 'purdue',\n",
       " 'yep',\n",
       " \"i'm\",\n",
       " 'still',\n",
       " 'in',\n",
       " 'london',\n",
       " 'which',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'awesome',\n",
       " 'p',\n",
       " 'remind',\n",
       " 'me',\n",
       " 'to',\n",
       " 'post',\n",
       " 'the',\n",
       " 'million',\n",
       " 'and',\n",
       " 'one',\n",
       " 'pictures',\n",
       " 'that',\n",
       " 'i',\n",
       " 'took',\n",
       " 'when',\n",
       " 'i',\n",
       " 'get',\n",
       " 'back',\n",
       " 'to',\n",
       " 'markham',\n",
       " 'have',\n",
       " 'to',\n",
       " 'say',\n",
       " 'i',\n",
       " 'hate',\n",
       " 'paris',\n",
       " \"hilton's\",\n",
       " 'behavior',\n",
       " 'but',\n",
       " 'i',\n",
       " 'do',\n",
       " 'think',\n",
       " \"she's\",\n",
       " 'kinda',\n",
       " 'cute',\n",
       " 'i',\n",
       " 'will',\n",
       " 'love',\n",
       " 'the',\n",
       " 'lakers',\n",
       " \"i'm\",\n",
       " 'so',\n",
       " 'glad',\n",
       " 'i',\n",
       " 'love',\n",
       " 'paris',\n",
       " 'hilton',\n",
       " 'too',\n",
       " 'or',\n",
       " 'this',\n",
       " 'would',\n",
       " 'be',\n",
       " 'excruciating',\n",
       " 'considering',\n",
       " 'most',\n",
       " 'geico',\n",
       " 'commericals',\n",
       " 'are',\n",
       " 'stupid',\n",
       " 'i',\n",
       " 'liked',\n",
       " 'mit',\n",
       " 'though',\n",
       " 'esp',\n",
       " 'their',\n",
       " 'little',\n",
       " 'info',\n",
       " 'book',\n",
       " 'before',\n",
       " 'i',\n",
       " 'left',\n",
       " 'missouri',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'london',\n",
       " 'was',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'so',\n",
       " 'good',\n",
       " 'and',\n",
       " 'cool',\n",
       " 'and',\n",
       " 'fun',\n",
       " 'and',\n",
       " 'a',\n",
       " 'really',\n",
       " 'great',\n",
       " 'experience',\n",
       " 'and',\n",
       " 'i',\n",
       " 'was',\n",
       " 'really',\n",
       " 'excited',\n",
       " 'i',\n",
       " 'still',\n",
       " 'like',\n",
       " 'tom',\n",
       " 'cruise',\n",
       " 'well',\n",
       " 'i',\n",
       " 'had',\n",
       " 'a',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'crap',\n",
       " 'toyota',\n",
       " 'celica',\n",
       " 'but',\n",
       " 'it',\n",
       " 'died',\n",
       " 'in',\n",
       " 'portland',\n",
       " 'and',\n",
       " 'i',\n",
       " 'got',\n",
       " 'a',\n",
       " 'ford',\n",
       " 'ranger',\n",
       " 'i',\n",
       " 'love',\n",
       " 'angelina',\n",
       " 'jolie',\n",
       " 'i',\n",
       " 'still',\n",
       " 'like',\n",
       " 'tom',\n",
       " 'cruise',\n",
       " 'ucla',\n",
       " 'is',\n",
       " 'beautiful',\n",
       " 'i',\n",
       " 'think',\n",
       " 'angelina',\n",
       " 'jolie',\n",
       " 'is',\n",
       " 'so',\n",
       " 'much',\n",
       " 'more',\n",
       " 'beautiful',\n",
       " 'than',\n",
       " 'jennifer',\n",
       " 'anniston',\n",
       " 'who',\n",
       " 'by',\n",
       " 'the',\n",
       " 'way',\n",
       " 'is',\n",
       " 'majorly',\n",
       " 'overrated',\n",
       " 'angelina',\n",
       " 'jolie',\n",
       " 'is',\n",
       " 'beautiful',\n",
       " 'and',\n",
       " \"honda's\",\n",
       " 'are',\n",
       " 'awesome',\n",
       " 'i',\n",
       " 'love',\n",
       " 'harvard',\n",
       " 'i',\n",
       " 'love',\n",
       " 'tom',\n",
       " 'cruise',\n",
       " 'this',\n",
       " 'means',\n",
       " 'we',\n",
       " 'beat',\n",
       " 'out',\n",
       " 'schools',\n",
       " 'like',\n",
       " 'mit',\n",
       " 'which',\n",
       " 'is',\n",
       " 'amazing',\n",
       " 'for',\n",
       " 'a',\n",
       " 'relatively',\n",
       " 'small',\n",
       " 'unassuming',\n",
       " \"lil'is\",\n",
       " 'department',\n",
       " 'i',\n",
       " 'hate',\n",
       " 'london',\n",
       " 'bugs',\n",
       " 'way',\n",
       " 'to',\n",
       " 'go',\n",
       " 'stupid',\n",
       " 'lakers',\n",
       " 'london',\n",
       " 'sucks',\n",
       " 'anyway',\n",
       " 'shanghai',\n",
       " 'is',\n",
       " 'really',\n",
       " 'beautiful',\n",
       " '\\xbc',\n",
       " '\\xbb\\xa8',\n",
       " '\\xb1\\x9f',\n",
       " '\\xa4\\xa7',\n",
       " '\\x93',\n",
       " '\\x98\\xaf',\n",
       " '\\xbe',\n",
       " '\\xb5',\n",
       " '\\x9a',\n",
       " '\\x95\\xa6',\n",
       " '\\xbc',\n",
       " '\\xa3',\n",
       " '\\xb8\\xaastarbucks',\n",
       " '\\x98\\xaf',\n",
       " '\\xb8',\n",
       " '\\xb5\\xb7',\n",
       " '\\xa3',\n",
       " '\\x99\\xaf',\n",
       " '\\x9c',\n",
       " '\\xa5\\xbd',\n",
       " '\\x9a',\n",
       " 'starbucks',\n",
       " 'buy',\n",
       " 'quite',\n",
       " 'a',\n",
       " 'few',\n",
       " 'food',\n",
       " 'to',\n",
       " 'back',\n",
       " 'to',\n",
       " 'notts',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'la',\n",
       " 'aiiii',\n",
       " 'notts',\n",
       " 'only',\n",
       " 'hv',\n",
       " '1',\n",
       " 'chinese',\n",
       " 'shop',\n",
       " 'in',\n",
       " 'town',\n",
       " 'ja',\n",
       " 'so',\n",
       " 'shit',\n",
       " 'london',\n",
       " 'is',\n",
       " 'so',\n",
       " 'great',\n",
       " 'angelina',\n",
       " 'jolie',\n",
       " 'is',\n",
       " 'so',\n",
       " 'beautiful',\n",
       " 'that',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'even',\n",
       " 'have',\n",
       " 'the',\n",
       " 'desire',\n",
       " 'to',\n",
       " 'attain',\n",
       " 'such',\n",
       " 'exquisite',\n",
       " 'beauty',\n",
       " 'i',\n",
       " 'reallllllly',\n",
       " 'hate',\n",
       " 'tom',\n",
       " 'cruise',\n",
       " 'to',\n",
       " 'my',\n",
       " 'understanding',\n",
       " 'harvard',\n",
       " 'is',\n",
       " 'a',\n",
       " 'very',\n",
       " 'difficult',\n",
       " 'college',\n",
       " 'to',\n",
       " 'get',\n",
       " 'in',\n",
       " 'to',\n",
       " 'boston',\n",
       " 'can',\n",
       " 'suck',\n",
       " 'my',\n",
       " 'fucking',\n",
       " 'tits',\n",
       " 'i',\n",
       " 'loved',\n",
       " 'boston',\n",
       " 'and',\n",
       " 'mit',\n",
       " 'so',\n",
       " 'much',\n",
       " 'and',\n",
       " 'still',\n",
       " 'do',\n",
       " 'i',\n",
       " 'like',\n",
       " 'honda',\n",
       " 'civics',\n",
       " 'no',\n",
       " 'matter',\n",
       " 'how',\n",
       " 'obvious',\n",
       " 'it',\n",
       " 'is',\n",
       " 'to',\n",
       " 'me',\n",
       " 'that',\n",
       " 'george',\n",
       " 'w',\n",
       " 'bush',\n",
       " 'is',\n",
       " 'an',\n",
       " 'arrogant',\n",
       " 'idiot',\n",
       " 'liar',\n",
       " 'there',\n",
       " 'are',\n",
       " 'plenty',\n",
       " 'of',\n",
       " 'people',\n",
       " 'who',\n",
       " 'believe',\n",
       " 'him',\n",
       " 'i',\n",
       " 'love',\n",
       " 'the',\n",
       " 'los',\n",
       " 'angeles',\n",
       " 'lakers',\n",
       " 'the',\n",
       " 'stupid',\n",
       " 'honda',\n",
       " 'lol',\n",
       " 'or',\n",
       " 'a',\n",
       " 'bug',\n",
       " 'seattle',\n",
       " 'sucks',\n",
       " 'anyways',\n",
       " 'i',\n",
       " 'want',\n",
       " 'a',\n",
       " 'thinkpad',\n",
       " 'or',\n",
       " 'something',\n",
       " 'i',\n",
       " 'love',\n",
       " 'shanghai',\n",
       " \"it's\",\n",
       " 'such',\n",
       " 'a',\n",
       " 'great',\n",
       " 'city',\n",
       " 'and',\n",
       " 'hongzhou',\n",
       " 'is',\n",
       " 'only',\n",
       " 'a',\n",
       " 'two',\n",
       " 'hour',\n",
       " 'train',\n",
       " 'ride',\n",
       " 'away',\n",
       " 'from',\n",
       " 'it',\n",
       " 'my',\n",
       " 'purdue',\n",
       " 'cal',\n",
       " 'friends',\n",
       " 'are',\n",
       " 'awesome',\n",
       " 'shanghai',\n",
       " 'is',\n",
       " 'beautiful',\n",
       " 'london',\n",
       " 'sucks',\n",
       " 'i',\n",
       " 'love',\n",
       " 'my',\n",
       " 'new',\n",
       " 'macbook',\n",
       " 'i',\n",
       " 'love',\n",
       " 'my',\n",
       " 'new',\n",
       " 'macbook',\n",
       " \"i'd\",\n",
       " 'love',\n",
       " 'to',\n",
       " 'see',\n",
       " 'the',\n",
       " 'clips',\n",
       " 'and',\n",
       " 'lakers',\n",
       " 'in',\n",
       " 'the',\n",
       " 'second',\n",
       " 'round',\n",
       " 'though',\n",
       " 'the',\n",
       " 'winner',\n",
       " 'would',\n",
       " 'just',\n",
       " 'be',\n",
       " 'a',\n",
       " 'stepping',\n",
       " 'stone',\n",
       " 'for',\n",
       " 'the',\n",
       " 'mavs',\n",
       " 'or',\n",
       " 'spurs',\n",
       " 'awesome',\n",
       " 'diner',\n",
       " 'here',\n",
       " 'purdue',\n",
       " 'i',\n",
       " 'love',\n",
       " 'my',\n",
       " 'new',\n",
       " 'macbook',\n",
       " 'san',\n",
       " 'francisco',\n",
       " 'was',\n",
       " 'brilliant',\n",
       " 'sausalito',\n",
       " 'as',\n",
       " 'for',\n",
       " 'myself',\n",
       " \"i'm\",\n",
       " 'wanting',\n",
       " 'a',\n",
       " 'new',\n",
       " 'honda',\n",
       " 'element',\n",
       " 'with',\n",
       " 'a',\n",
       " 'prescription',\n",
       " 'windshield',\n",
       " 'i',\n",
       " 'think',\n",
       " 'if',\n",
       " 'i',\n",
       " 'hate',\n",
       " 'boston',\n",
       " 'ill',\n",
       " 'just',\n",
       " 'disappear',\n",
       " 'to',\n",
       " 'california',\n",
       " 'for',\n",
       " 'the',\n",
       " 'summers',\n",
       " 'i',\n",
       " 'love',\n",
       " 'the',\n",
       " 'toyota',\n",
       " 'prius',\n",
       " 'seattle',\n",
       " 'sucks',\n",
       " 'anyways',\n",
       " 'i',\n",
       " 'love',\n",
       " 'harvard',\n",
       " 'square',\n",
       " 'in',\n",
       " 'the',\n",
       " 'fall',\n",
       " 'i',\n",
       " 'want',\n",
       " 'a',\n",
       " 'tour',\n",
       " 'of',\n",
       " 'london',\n",
       " 'on',\n",
       " 'the',\n",
       " 'back',\n",
       " 'of',\n",
       " 'that',\n",
       " 'bike',\n",
       " 'i',\n",
       " 'know',\n",
       " \"you're\",\n",
       " 'way',\n",
       " 'too',\n",
       " 'smart',\n",
       " 'and',\n",
       " 'way',\n",
       " 'too',\n",
       " 'cool',\n",
       " 'to',\n",
       " 'let',\n",
       " 'stupid',\n",
       " 'ucla',\n",
       " 'get',\n",
       " 'to',\n",
       " 'you',\n",
       " 'the',\n",
       " 'lakers',\n",
       " 'are',\n",
       " 'playing',\n",
       " 'really',\n",
       " 'awesome',\n",
       " 'ball',\n",
       " 'it',\n",
       " \"isn't\",\n",
       " 'over',\n",
       " 'until',\n",
       " 'its',\n",
       " 'over',\n",
       " 'though',\n",
       " 'and',\n",
       " 'for',\n",
       " 'now',\n",
       " 'my',\n",
       " 'brain',\n",
       " 'is',\n",
       " 'set',\n",
       " 'on',\n",
       " 'i',\n",
       " 'love',\n",
       " 'harvard',\n",
       " 'i',\n",
       " 'want',\n",
       " 'to',\n",
       " 'come',\n",
       " 'to',\n",
       " 'harvard',\n",
       " 'mode',\n",
       " 'harvard',\n",
       " 'is',\n",
       " 'dumb',\n",
       " 'i',\n",
       " 'mean',\n",
       " 'they',\n",
       " 'really',\n",
       " 'have',\n",
       " 'to',\n",
       " 'be',\n",
       " 'stupid',\n",
       " 'to',\n",
       " 'have',\n",
       " 'not',\n",
       " 'wanted',\n",
       " 'her',\n",
       " 'to',\n",
       " 'be',\n",
       " 'at',\n",
       " 'their',\n",
       " 'school',\n",
       " \"i'm\",\n",
       " 'loving',\n",
       " 'shanghai',\n",
       " 'harvard',\n",
       " 'is',\n",
       " 'for',\n",
       " 'dumb',\n",
       " 'people',\n",
       " 'as',\n",
       " 'i',\n",
       " 'stepped',\n",
       " 'out',\n",
       " 'of',\n",
       " 'my',\n",
       " 'beautiful',\n",
       " 'toyota',\n",
       " 'i',\n",
       " 'heard',\n",
       " 'a',\n",
       " 'scream',\n",
       " 'as',\n",
       " 'a',\n",
       " 'gust',\n",
       " 'of',\n",
       " 'wind',\n",
       " 'had',\n",
       " 'just',\n",
       " 'passed',\n",
       " 'bodies',\n",
       " 'being',\n",
       " 'dismembered',\n",
       " 'blown',\n",
       " 'apart',\n",
       " 'and',\n",
       " 'mutilated',\n",
       " 'in',\n",
       " 'any',\n",
       " 'number',\n",
       " 'of',\n",
       " 'ways',\n",
       " 'or',\n",
       " 'angelina',\n",
       " 'jolie',\n",
       " 'sucking',\n",
       " 'a',\n",
       " 'dick',\n",
       " 'on',\n",
       " 'a',\n",
       " '30',\n",
       " 'foot',\n",
       " 'screen',\n",
       " 'i',\n",
       " 'love',\n",
       " 'harvard',\n",
       " 'square',\n",
       " 'in',\n",
       " 'the',\n",
       " 'fall',\n",
       " 'london',\n",
       " 'amazing',\n",
       " 'i',\n",
       " 'hate',\n",
       " 'london',\n",
       " 'i',\n",
       " 'love',\n",
       " 'mit',\n",
       " 'so',\n",
       " 'much',\n",
       " 'i',\n",
       " 'told',\n",
       " 'her',\n",
       " 'that',\n",
       " 'ucla',\n",
       " 'is',\n",
       " 'excellent',\n",
       " 'for',\n",
       " 'both',\n",
       " 'i',\n",
       " 'think',\n",
       " 'at',\n",
       " 'this',\n",
       " 'moment',\n",
       " 'i',\n",
       " 'love',\n",
       " 'san',\n",
       " 'francisco',\n",
       " 'better',\n",
       " 'than',\n",
       " 'l',\n",
       " 'a',\n",
       " 'i',\n",
       " 'think',\n",
       " 'angelina',\n",
       " 'jolie',\n",
       " 'is',\n",
       " 'so',\n",
       " 'much',\n",
       " 'more',\n",
       " 'beautiful',\n",
       " 'than',\n",
       " 'jennifer',\n",
       " 'anniston',\n",
       " 'who',\n",
       " 'by',\n",
       " 'the',\n",
       " 'way',\n",
       " 'is',\n",
       " 'majorly',\n",
       " 'overrated',\n",
       " 'i',\n",
       " 'also',\n",
       " 'love',\n",
       " 'boston',\n",
       " 'legal',\n",
       " 'the',\n",
       " 'stupid',\n",
       " 'honda',\n",
       " 'lol',\n",
       " 'or',\n",
       " 'a',\n",
       " 'bug',\n",
       " 'personally',\n",
       " 'i',\n",
       " 'blame',\n",
       " 'angelina',\n",
       " 'jolie',\n",
       " 'a',\n",
       " 'psychologically',\n",
       " 'damaged',\n",
       " 'attention',\n",
       " 'seeker',\n",
       " 'who',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'have',\n",
       " 'somehow',\n",
       " 'convinced',\n",
       " 'almost',\n",
       " 'every',\n",
       " 'woman',\n",
       " 'on',\n",
       " 'lj',\n",
       " 'that',\n",
       " 'having',\n",
       " 'an',\n",
       " 'icon',\n",
       " 'with',\n",
       " 'her',\n",
       " 'face',\n",
       " 'on',\n",
       " 'it',\n",
       " 'makes',\n",
       " 'you',\n",
       " 'a',\n",
       " 'sexually',\n",
       " 'interesting',\n",
       " 'noncomformist',\n",
       " 'wildchild',\n",
       " \"i've\",\n",
       " 'decided',\n",
       " 'i',\n",
       " 'really',\n",
       " 'miss',\n",
       " 'london',\n",
       " 'prolly',\n",
       " 'going',\n",
       " 'to',\n",
       " 'cambridge',\n",
       " 'on',\n",
       " 'tuesday',\n",
       " 'i',\n",
       " 'need',\n",
       " 'to',\n",
       " 'see',\n",
       " 'mit',\n",
       " 'to',\n",
       " 'survive',\n",
       " 'till',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'term',\n",
       " 'angelina',\n",
       " 'jolie',\n",
       " 'is',\n",
       " 'very',\n",
       " 'beautiful',\n",
       " 'yea',\n",
       " 'i',\n",
       " 'love',\n",
       " 'mit',\n",
       " 'and',\n",
       " 'i',\n",
       " 'really',\n",
       " 'dk',\n",
       " 'why',\n",
       " 'oh',\n",
       " 'traffic',\n",
       " 'in',\n",
       " 'seattle',\n",
       " 'sucks',\n",
       " \"that's\",\n",
       " 'why',\n",
       " 'i',\n",
       " 'most',\n",
       " 'love',\n",
       " 'the',\n",
       " 'harvard',\n",
       " 'story',\n",
       " 'i',\n",
       " 'love',\n",
       " 'ucla',\n",
       " 'but',\n",
       " 'miss',\n",
       " 'everyone',\n",
       " 'from',\n",
       " 'back',\n",
       " 'home',\n",
       " 'paris',\n",
       " 'hilton',\n",
       " 'sucks',\n",
       " 'tough',\n",
       " 'game',\n",
       " 'to',\n",
       " 'pick',\n",
       " 'but',\n",
       " \"i'll\",\n",
       " 'take',\n",
       " 'seattle',\n",
       " 'at',\n",
       " 'home',\n",
       " 'with',\n",
       " 'the',\n",
       " 'mobile',\n",
       " 'qb',\n",
       " 'and',\n",
       " 'matt',\n",
       " 'morris',\n",
       " 'against',\n",
       " 'a',\n",
       " 'pitiful',\n",
       " 'st',\n",
       " 'louis',\n",
       " 'run',\n",
       " 'defense',\n",
       " 'anyway',\n",
       " 'some',\n",
       " 'crappy',\n",
       " 'honda',\n",
       " 'but',\n",
       " 'as',\n",
       " 'is',\n",
       " 'young',\n",
       " \"peoples'customs\",\n",
       " 'it',\n",
       " 'has',\n",
       " 'a',\n",
       " 'good',\n",
       " '2',\n",
       " 'million',\n",
       " 'worth',\n",
       " 'of',\n",
       " 'sound',\n",
       " 'equipment',\n",
       " 'inside',\n",
       " 'sticking',\n",
       " 'a',\n",
       " 'nerf',\n",
       " 'football',\n",
       " 'on',\n",
       " \"lennon's\",\n",
       " 'ugly',\n",
       " 'toyota',\n",
       " 'kissing',\n",
       " 'the',\n",
       " 'audi',\n",
       " 'ummm',\n",
       " 'i',\n",
       " 'love',\n",
       " 'you',\n",
       " 'my',\n",
       " 'harvard',\n",
       " 'boy',\n",
       " 'i',\n",
       " 'also',\n",
       " 'love',\n",
       " 'the',\n",
       " 'new',\n",
       " 'rabbits',\n",
       " 'i',\n",
       " 'still',\n",
       " 'want',\n",
       " 'an',\n",
       " 'x',\n",
       " 'terra',\n",
       " 'but',\n",
       " \"luke's\",\n",
       " 'will',\n",
       " 'do',\n",
       " 'and',\n",
       " 'i',\n",
       " 'kinda',\n",
       " 'like',\n",
       " 'honda',\n",
       " 'elements',\n",
       " 'but',\n",
       " 'they',\n",
       " 'got',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'safety',\n",
       " 'rating',\n",
       " 'oh',\n",
       " 'my',\n",
       " 'god',\n",
       " 'i',\n",
       " 'love',\n",
       " 'pommes',\n",
       " 'mit',\n",
       " 'mayo',\n",
       " 'i',\n",
       " 'love',\n",
       " 'angelina',\n",
       " 'jolie',\n",
       " 'i',\n",
       " 'know',\n",
       " \"you're\",\n",
       " 'way',\n",
       " 'too',\n",
       " 'smart',\n",
       " 'and',\n",
       " 'way',\n",
       " 'too',\n",
       " 'cool',\n",
       " 'to',\n",
       " 'let',\n",
       " 'stupid',\n",
       " 'ucla',\n",
       " 'get',\n",
       " 'to',\n",
       " 'you',\n",
       " \"i'm\",\n",
       " 'not',\n",
       " 'crazy',\n",
       " 'about',\n",
       " 'hk',\n",
       " 'either',\n",
       " 'but',\n",
       " 'shanghai',\n",
       " 'is',\n",
       " 'sounding',\n",
       " 'awesome',\n",
       " 'i',\n",
       " 'love',\n",
       " 'the',\n",
       " 'toyota',\n",
       " 'prius',\n",
       " '\\x91',\n",
       " '\\x9c\\xa8',\n",
       " '\\x94\\xa8that',\n",
       " 'stupid',\n",
       " 'box',\n",
       " 'car',\n",
       " 'honda',\n",
       " 'crx',\n",
       " 'o',\n",
       " 'crap',\n",
       " 'game',\n",
       " 'i',\n",
       " 'hate',\n",
       " 'paris',\n",
       " 'hilton',\n",
       " \"aaa's\",\n",
       " 'q',\n",
       " 'is',\n",
       " 'catchy',\n",
       " 'and',\n",
       " 'an',\n",
       " 'ear',\n",
       " 'worm',\n",
       " 'like',\n",
       " 'many',\n",
       " 'of',\n",
       " 'the',\n",
       " 'things',\n",
       " \"they've\",\n",
       " 'done',\n",
       " 'i',\n",
       " 'need',\n",
       " 'to',\n",
       " 'pay',\n",
       " 'geico',\n",
       " 'and',\n",
       " 'a',\n",
       " 'host',\n",
       " 'of',\n",
       " 'other',\n",
       " 'bills',\n",
       " 'but',\n",
       " 'that',\n",
       " 'is',\n",
       " 'neither',\n",
       " 'here',\n",
       " 'nor',\n",
       " 'there',\n",
       " 'shanghai',\n",
       " 'is',\n",
       " 'beautiful',\n",
       " 'and',\n",
       " 'as',\n",
       " 'stupid',\n",
       " 'as',\n",
       " 'san',\n",
       " \"francisco's\",\n",
       " 'road',\n",
       " 'system',\n",
       " 'is',\n",
       " 'we',\n",
       " ...]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfinder = collocations.BigramCollocationFinder.from_words(text_words)\n",
    "cfinder.apply_freq_filter(5)\n",
    "measure = collocations.BigramAssocMeasures.raw_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'love'), ('i', 'hate'), ('paris', 'hilton'), ('tom', 'cruise'), ('san', 'francisco'), ('angelina', 'jolie'), ('and', 'i'), ('love', 'the'), ('the', 'lakers'), ('i', 'like'), ('love', 'my'), ('i', 'think'), ('but', 'i'), ('hate', 'paris'), ('that', 'i'), ('to', 'be'), ('hate', 'tom'), ('i', 'want'), ('i', 'miss'), ('i', 'need'), ('so', 'much'), ('love', 'paris'), ('of', 'the'), ('seattle', 'sucks'), ('i', 'really'), ('jolie', 'is'), ('stupid', 'ucla'), ('sucks', 'i'), ('is', 'beautiful'), ('by', 'the'), ('in', 'the'), ('hate', 'london'), ('is', 'awesome'), ('i', 'still'), ('love', 'angelina'), ('to', 'go'), ('as', 'i'), ('i', 'have'), ('is', 'a'), ('i', 'can'), ('in', 'my'), ('in', 'seattle'), ('to', 'see'), ('cruise', 'i'), ('to', 'the'), ('hate', 'the'), ('hilton', 'i'), ('stupid', 'lakers'), ('of', 'that'), ('love', 'ucla'), ('which', 'is'), ('is', 'so'), ('awesome', 'i'), ('want', 'a'), ('my', 'macbook'), ('love', 'seattle'), ('love', 'shanghai'), ('the', 'way'), ('like', 'the'), ('francisco', 'is'), ('london', 'i'), ('i', 'loved'), ('i', 'was'), ('ucla', 'is'), ('of', 'my'), ('love', 'tom'), ('the', 'world'), ('think', 'angelina'), ('my', 'beautiful'), ('going', 'to'), ('like', 'tom'), ('at', 'harvard'), ('so', 'i'), ('some', 'of'), ('need', 'some'), ('that', 'geico'), ('balboa', 'stuff'), ('geico', 'balboa'), ('seattle', 'i'), ('when', 'i'), ('ucla', 'i'), ('and', 'the'), ('shanghai', 'is'), ('in', 'a'), ('a', 'lot'), ('i', 'also'), ('much', 'as'), ('macbook', 'pro'), ('hillary', 'clinton'), ('i', 'had'), ('as', 'much'), ('has', 'been'), ('boston', 'sucks'), ('but', 'the'), ('out', 'of'), ('more', 'than'), ('would', 'be'), ('on', 'the'), ('love', 'you'), ('much', 'i')]\n"
     ]
    }
   ],
   "source": [
    "print(cfinder.nbest(measure,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    features = []\n",
    "    \n",
    "    <bag of words для коллокаций>\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "X = []\n",
    "for text in texts:\n",
    "    X.append(extract_features(text))\n",
    "X = numpy.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = <обучите модель>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred_train = <предскажите вероятности для обучающей выборки copy-paste>\n",
    "Y_pred_test = <предскажите вероятности для обучающей выборки copy-paste>\n",
    "print(\"Обучение:\",roc_auc_score(Y_train,Y_pred_train))\n",
    "print(\"Тест:\",roc_auc_score(Y_test,Y_pred_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
